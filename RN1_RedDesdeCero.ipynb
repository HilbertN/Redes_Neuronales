{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy7wzjpH7acakoAqNu+NpK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HilbertN/Redes_Neuronales/blob/main/RN1_RedDesdeCero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python\n"
      ],
      "metadata": {
        "id": "eQWYvQ4pZ5p_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clases"
      ],
      "metadata": {
        "id": "QNBAcBaC6Q6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjuJ0wnE6MhH"
      },
      "outputs": [],
      "source": [
        "class Humano:\n",
        "  def __init__(self,edad): #Aquí definimos los atributos del objeto\n",
        "    self.edad=edad\n",
        "\n",
        "#Después definimos los métodos\n",
        "  def hablar(self,mensaje):\n",
        "    print(mensaje )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pedro=Humano(26)\n",
        "juan=Humano(30)"
      ],
      "metadata": {
        "id": "PMOWN9uK7CrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"soy pedro y tengo\",pedro.edad,\"de edad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myr_i3EB7I1Q",
        "outputId": "fde75c01-e2d6-4c47-f1f3-d8fcd9d5ea66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "soy pedro y tengo 26 de edad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "juan.hablar('Hola')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du0t3gl_7U-h",
        "outputId": "7e4bfcd9-f333-45bf-ce45-674b7e33c506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Herencia"
      ],
      "metadata": {
        "id": "nIa0N7Jn7a53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#El ingeniero en sistemas y el licenciado de derecho siguen siendo humanos entonces\n",
        "#queremos que tengan los mismos atributos y métodos que tiene la clase humano.\n",
        "\n",
        "class IngSistemas(Humano):\n",
        "  def lenguaje(self,programa):\n",
        "    print(\"voya a programar en\", programa)\n",
        "Pedro=IngSistemas(26)\n",
        "\n",
        "#Si creamos un __init__ especial para la clase Liceciando en derecho entonces\n",
        "#no considera el __init_ de la clase humana, es decir, ya no hereda los atributos\n",
        "#de la clase humana y solo considera los de la subclase.\n",
        "\n",
        "#Observación importante: debemos tener en cuenta que ya no considera los atributos\n",
        "#de la clase Humano, pero sí los métodos entonces si un método hace uso de alguno de\n",
        "#los atributos de esta clase entonces vamos a tener un error.\n",
        "\n",
        "class LicDerecho(Humano):\n",
        "  def __init__(self):\n",
        "    print('Yo te puedo divociar sin firma de tu cónyuge')\n",
        "  def estudiarcaso(self,de):\n",
        "    print(\"debo estudiar el caso de\", de)\n",
        "\n"
      ],
      "metadata": {
        "id": "bTy5hKCR7caX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Observamos que pedro contiene los métodos que tiene la clase humano\n",
        "Pedro.hablar('Hola juan')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IQZi97s9VOj",
        "outputId": "af46acb0-15bf-4214-8e3d-da3c94ba6e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola juan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Juan=LicDerecho()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeisID8q_myG",
        "outputId": "5b8be645-2242-454c-dcf7-327a34ca04b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yo te puedo divociar sin firma de tu cónyuge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Herencia múltiple\n"
      ],
      "metadata": {
        "id": "uBtebykIY-PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Podemos crear una clase que tenga los métodos de dos o más clases de la siguiente forma\n",
        "class Estudioso(IngSistemas,LicDerecho):\n",
        "  pass\n",
        "#Observaciones importantes:\n",
        "#Cuando creamos una clase debe haber métodos, para evitar escribir algun método podemos escribir 'pass'.\n",
        "#Los atributos (método que se forma con __init__) los va a tomar de la primera Clase entonces el orden es importante.\n",
        "#Primero considera las clases que coloca a en parátensis y luego si hay clases de herencia."
      ],
      "metadata": {
        "id": "1Qtb-m5rZA2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Oscar=Estudioso()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE0svnM6aPSx",
        "outputId": "110ee540-17ce-4fb1-abaf-4ffd02fab75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yo te puedo divociar sin firma de tu cónyuge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes neuronales (Código Clase)"
      ],
      "metadata": {
        "id": "v5fmjeiwbdH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por cierto, cuando describí los datos de MNIST anteriormente, mencioné que estaban divididos en 60,000 imágenes de entrenamiento y 10,000 imágenes de prueba. Esa es la descripción oficial de MNIST. En realidad, vamos a dividir los datos de manera un poco diferente. Dejaremos las imágenes de prueba como están, pero dividiremos el conjunto de entrenamiento de MNIST de 60,000 imágenes en dos partes: un conjunto de 50,000 imágenes que utilizaremos para entrenar nuestra red neuronal, y un conjunto de validación separado de 10,000 imágenes. No utilizaremos los datos de validación en este capítulo, pero más adelante en el libro nos serán útiles para determinar cómo ajustar ciertos hiperparámetros de la red neuronal, como la tasa de aprendizaje, y otros aspectos que no son seleccionados directamente por nuestro algoritmo de aprendizaje. Aunque los datos de validación no formen parte de la especificación original de MNIST, muchas personas utilizan MNIST de esta manera, y el uso de datos de validación es común en las redes neuronales. A partir de ahora, cuando me refiera a los \"datos de entrenamiento de MNIST\", estaré hablando de nuestro conjunto de datos de 50,000 imágenes, no del conjunto original de 60,000 imágenes*."
      ],
      "metadata": {
        "id": "HK_w5rMrcKFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permítanme explicar las características principales del código de las redes neuronales antes de proporcionar el listado completo que se encuentra más abajo. El elemento central es una clase llamada \"Network\" (Red), que utilizamos para representar una red neuronal. Aquí está el código que utilizamos para inicializar un objeto de tipo \"Network\":"
      ],
      "metadata": {
        "id": "dcM5NvoYcnTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Network(object):\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]"
      ],
      "metadata": {
        "id": "u7nLtMf9bbn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los sesgos (biases) y los pesos (weights) en el objeto de la red (Network) se inicializan aleatoriamente, utilizando la función np.random.randn de Numpy para generar distribuciones gaussianas con media 0 y desviación estándar 1. Esta inicialización aleatoria brinda a nuestro algoritmo de descenso de gradiente estocástico un punto de partida. En capítulos posteriores encontraremos mejores formas de inicializar los pesos y sesgos, pero esto servirá por ahora. Es importante destacar que el código de inicialización de la red asume que la primera capa de neuronas es la capa de entrada (input layer) y omite establecer cualquier sesgo para esas neuronas, ya que los sesgos solo se utilizan para calcular las salidas de las capas posteriores."
      ],
      "metadata": {
        "id": "49oJLuJsdeLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "También es importante señalar que los sesgos (biases) y los pesos (weights) se almacenan como listas de matrices de Numpy. Por ejemplo, net.weights[1] es una matriz de Numpy que almacena los pesos que conectan la segunda y tercera capa de neuronas. (No es la primera y segunda capa, ya que la indexación de listas en Python comienza en 0). Dado que net.weights[1] es bastante largo, simplemente denotemos esa matriz como \"w\". Esta matriz es tal que wjk es el peso para la conexión entre la k-ésima neurona en la segunda capa y la j-ésima neurona en la tercera capa. El ordenamiento de los índices j y k puede parecer extraño; ¿no tendría más sentido intercambiar los índices j y k? La gran ventaja de usar este ordenamiento es que significa que el vector de activaciones de la tercera capa de neuronas es:\n",
        "a′=σ(wa+b).(22)\n",
        "En esta ecuación hay bastante sucediendo, así que vamos a desglosarla pieza por pieza. \"a\" es el vector de activaciones de la segunda capa de neuronas. Para obtener \"a′\", multiplicamos \"a\" por la matriz de pesos \"w\" y sumamos el vector de sesgos \"b\". Luego, aplicamos la función \"σ\" elemento por elemento a cada entrada en el vector \"wa+b\". (Esto se llama vectorizar la función \"σ\").\n"
      ],
      "metadata": {
        "id": "_LdiOwYPeVCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con todo esto en mente, es fácil escribir código para calcular la salida de una instancia de red (Network). Comenzamos definiendo la función sigmoide:"
      ],
      "metadata": {
        "id": "lGn_rP9NeoXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))"
      ],
      "metadata": {
        "id": "GxHxRYeUerY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten en cuenta que cuando la entrada \"z\" es un vector o una matriz de Numpy, Numpy aplica automáticamente la función sigmoide elemento por elemento, es decir, en forma vectorizada."
      ],
      "metadata": {
        "id": "E4vKozc5ewL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego agregamos un método de propagación hacia adelante (feedforward) a la clase Network, que, dado una entrada \"a\" para la red, devuelve la salida correspondiente."
      ],
      "metadata": {
        "id": "9OYQt5pdfIxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def feedforward(self, a):\n",
        "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a"
      ],
      "metadata": {
        "id": "oijiwLjcfIVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            test_data=None):\n",
        "#Entrena la red neuronal utilizando el descenso de gradiente estocástico en mini lotes.\n",
        "#Los \"training_data\" son una lista de tuplas \"(x, y)\" que representan las entradas de entrenamiento y\n",
        "#las salidas deseadas. Los otros parámetros no opcionales son autoexplicativos. Si se proporciona \"test_data\",\n",
        "#la red será evaluada con respecto a los datos de prueba después de cada época, y se mostrará el progreso parcial.\n",
        "#Esto es útil para hacer un seguimiento del progreso, pero ralentiza las cosas sustancialmente.\n",
        "        if test_data: n_test = len(test_data)\n",
        "        n = len(training_data)\n",
        "        for j in xrange(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in xrange(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print (\"Epoch {0}: {1} / {2}\").format(\n",
        "                    j, self.evaluate(test_data), n_test)\n",
        "            else:\n",
        "                print (\"Epoch {0} complete\").format(j)"
      ],
      "metadata": {
        "id": "WXBQH2gPfvgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código funciona de la siguiente manera. En cada época, comienza aleatoriamente reordenando los datos de entrenamiento y luego los divide en mini lotes del tamaño adecuado. Esta es una forma sencilla de muestrear aleatoriamente los datos de entrenamiento. Luego, para cada mini_lote, aplicamos un solo paso de descenso de gradiente. Esto se hace con el código self.update_mini_batch(mini_lote, eta), que actualiza los pesos y sesgos de la red según una sola iteración del descenso de gradiente, utilizando solo los datos de entrenamiento en el mini_lote. Aquí está el código para el método update_mini_batch:"
      ],
      "metadata": {
        "id": "myrMIAePi9rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        #Actualiza los pesos y sesgos de la red aplicando el descenso de gradiente utilizando\n",
        "        #la retropropagación (backpropagation) en un solo mini lote.\n",
        "        #El \"mini_lote\" es una lista de tuplas \"(x, y)\", y \"eta\" es la tasa de aprendizaje (learning rate).\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]"
      ],
      "metadata": {
        "id": "ByiGn4pGkn9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Qué tan bien reconoce el programa los dígitos escritos a mano? Bueno, empecemos por cargar los datos de MNIST. Haré esto utilizando un pequeño programa auxiliar llamado \"mnist_loader.py\", que se describirá a continuación. Ejecutamos los siguientes comandos en una consola de Python:"
      ],
      "metadata": {
        "id": "eqqUKM4Tkncr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import mnist_loader\n",
        "#training_data, validation_data, test_data =  mnist_loader.load_data_wrapper()"
      ],
      "metadata": {
        "id": "5QNRm7pqwHPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de cargar los datos de MNIST, configuraremos una red neuronal con 30 neuronas ocultas. Hacemos esto después de importar el programa Python mencionado anteriormente, que se llama \"network\"."
      ],
      "metadata": {
        "id": "YNipZzBtwoob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import network --> se importa el network porque se supone que esta en otro archivo de python\n",
        "#net = network.Network([784, 30, 10])"
      ],
      "metadata": {
        "id": "6-uzbiVPwt2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, utilizaremos el descenso de gradiente estocástico para aprender a partir de los datos de entrenamiento de MNIST durante 30 épocas, con un tamaño de mini lote de 10 y una tasa de aprendizaje de η=3.0."
      ],
      "metadata": {
        "id": "X7nzwY1NxGYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
      ],
      "metadata": {
        "id": "JB_LUsvfxIDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anteriormente, omití los detalles sobre cómo se cargan los datos de MNIST. Es bastante sencillo. Para completar la información, aquí tienes el código. Las estructuras de datos utilizadas para almacenar los datos de MNIST se describen en las cadenas de documentación; es algo sencillo, tuplas y listas de objetos Numpy ndarray (piensa en ellos como vectores si no estás familiarizado con ndarrays):"
      ],
      "metadata": {
        "id": "jg7dESfYx3M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una librería para cargar los datos de imágenes de MNIST. Para obtener detalles sobre las estructuras de datos que se devuelven, consulta las cadenas de documentación de las funciones \"load_data\" y \"load_data_wrapper\". En la práctica, la función \"load_data_wrapper\" es la que generalmente se llama en nuestro código de red neuronal."
      ],
      "metadata": {
        "id": "AbMnmxS8yO6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle # cPickle Esta libreria ya no esta disponible en python 3, en su lugar esta pickle\n",
        "import gzip\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "udlHidxoyZgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Devuelve los datos de MNIST como una tupla que contiene los datos de entrenamiento, los datos de validación y los datos de prueba.\n",
        "\n",
        "Los \"training_data\" se devuelven como una tupla con dos entradas. La primera entrada contiene las imágenes de entrenamiento reales. Esto es un ndarray de Numpy con 50,000 entradas. Cada entrada es, a su vez, un ndarray de Numpy con 784 valores, que representan los 28 * 28 = 784 píxeles en una sola imagen de MNIST.\n",
        "\n",
        "La segunda entrada en la tupla \"training_data\" es un ndarray de Numpy que contiene 50,000 entradas. Esas entradas son simplemente los valores de los dígitos (0...9) para las imágenes correspondientes contenidas en la primera entrada de la tupla.\n",
        "\n",
        "Los \"validation_data\" y \"test_data\" son similares, excepto que cada uno contiene solo 10,000 imágenes.\n",
        "\n",
        "Este es un buen formato de datos, pero para su uso en redes neuronales, es útil modificar un poco el formato de los \"training_data\". Eso se hace en la función auxiliar \"load_data_wrapper()\", como se muestra a continuación."
      ],
      "metadata": {
        "id": "vbP1xRNdzOWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
        "    training_data, validation_data, test_data = pickle.load(f)# se cambia cPickle.load(f) por pickle.load(f)\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)\n"
      ],
      "metadata": {
        "id": "QFCs5d68y64u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Devuelve una tupla que contiene (training_data, validation_data, test_data). Está basado en la función \"load_data\", pero el formato es más conveniente para su uso en nuestra implementación de redes neuronales.\n",
        "\n",
        "En particular, \"training_data\" es una lista que contiene 50,000 tuplas (x, y). \"x\" es un ndarray de Numpy de 784 dimensiones que contiene la imagen de entrada. \"y\" es un ndarray de Numpy de 10 dimensiones que representa el vector unitario correspondiente al dígito correcto para \"x\".\n",
        "\n",
        "\"validation_data\" y \"test_data\" son listas que contienen 10,000 tuplas (x, y). En cada caso, \"x\" es un ndarray de Numpy de 784 dimensiones que contiene la imagen de entrada, y \"y\" es la clasificación correspondiente, es decir, los valores de los dígitos (enteros) correspondientes a \"x\".\n",
        "\n",
        "Obviamente, esto significa que estamos utilizando formatos ligeramente diferentes para los datos de entrenamiento y los datos de validación / prueba. Estos formatos resultan ser los más convenientes para su uso en nuestro código de red neuronal."
      ],
      "metadata": {
        "id": "HRkJXzq8zyh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "uoDm-XU_zm-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Devuelve un vector unitario de 10 dimensiones con un valor de 1.0 en la posición j-ésima y ceros en las demás posiciones. Esto se utiliza para convertir un dígito (0...9) en una salida deseada correspondiente de la red neuronal."
      ],
      "metadata": {
        "id": "Slpx-5EUz8wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorized_result(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ],
      "metadata": {
        "id": "ErgRrI74z15X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmo completo:"
      ],
      "metadata": {
        "id": "go-oDtknyLrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %load network.py\n",
        "\n",
        "\"\"\"\n",
        "network.py\n",
        "~~~~~~~~~~\n",
        "IT WORKS\n",
        "\n",
        "A module to implement the stochastic gradient descent learning\n",
        "algorithm for a feedforward neural network.  Gradients are calculated\n",
        "using backpropagation.  Note that I have focused on making the code\n",
        "simple, easily readable, and easily modifiable.  It is not optimized,\n",
        "and omits many desirable features.\n",
        "\"\"\"\n",
        "\n",
        "#### Libraries\n",
        "# Standard library\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"La lista sizes contiene el número de neuronas en las capas\n",
        "        respectivas de la red neuronal. Por ejemplo, si la lista fuera\n",
        "        [2, 3, 1], entonces sería una red de tres capas, con la primera\n",
        "        capa conteniendo 2 neuronas, la segunda capa 3 neuronas y la\n",
        "        tercera capa 1 neurona. Los sesgos (biases) y pesos (weights) de\n",
        "        la red se inicializan de forma aleatoria, utilizando una\n",
        "        distribución Gaussiana con media 0 y varianza 1.\n",
        "        Es importante destacar que se asume que la primera\n",
        "        capa es una capa de entrada y, por convención, no estableceremos\n",
        "        ningún sesgo para esas neuronas, ya que los sesgos solo se\n",
        "        utilizan para calcular las salidas de las capas posteriores.\"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Devuelve la salida de la red si a es la entrada.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            test_data=None):\n",
        "        \"\"\"Entrena la red neuronal utilizando el descenso de gradiente\n",
        "        estocástico por lotes pequeños (mini-batch). Los datos de\n",
        "        entrenamiento, representados por la lista de tuplas (x, y),\n",
        "        contienen las entradas de entrenamiento y las salidas deseadas.\n",
        "        Los demás parámetros que no son opcionales son autoexplicativos.\n",
        "        Si se proporciona test_data, entonces la red se evaluará con\n",
        "        respecto a los datos de prueba después de cada época y se\n",
        "        imprimirá el progreso parcial. Esto es útil para hacer un\n",
        "        seguimiento del progreso, pero ralentiza sustancialmente el\n",
        "        proceso. En resumen, esta función entrena la red neuronal\n",
        "        utilizando el método de descenso de gradiente estocástico por\n",
        "        lotes pequeños, ajustando los pesos y sesgos de la red en función\n",
        "        de los datos de entrenamiento proporcionados. Además,\n",
        "        si se proporcionan datos de prueba, se evalúa el rendimiento\n",
        "        de la red en los datos de prueba después de cada época para\n",
        "        realizar un seguimiento de su mejora a lo largo del tiempo.\"\"\"\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Actualiza los pesos y sesgos de la red aplicando el descenso\n",
        "        de gradiente utilizando la retropropagación (backpropagation) a\n",
        "        un solo mini lote. El parámetro mini_batch es una lista de tuplas\n",
        "        (x, y), donde x representa las entradas y y las salidas deseadas\n",
        "        correspondientes a ese mini lote. El parámetro eta representa\n",
        "        la tasa de aprendizaje (learning rate).\n",
        "        \"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Devuelve una tupla (nabla_b, nabla_w) que representa el\n",
        "        gradiente para la función de costo C_x. nabla_b y nabla_w son\n",
        "        listas de matrices de numpy, similares a self.biases y self.weights,\n",
        "        pero se refieren al gradiente de la función de costo en relación\n",
        "        con los sesgos y pesos de la red neuronal, respectivamente..\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Note that the variable l in the loop below is used a little\n",
        "        # differently to the notation in Chapter 2 of the book.  Here,\n",
        "        # l = 1 means the last layer of neurons, l = 2 is the\n",
        "        # second-last layer, and so on.  It's a renumbering of the\n",
        "        # scheme in the book, used here to take advantage of the fact\n",
        "        # that Python can use negative indices in lists.\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Devuelve el número de entradas de prueba para las cuales la red\n",
        "        neuronal produce el resultado correcto. Se asume que la salida de\n",
        "        la red neuronal es el índice de la neurona en la capa final que\n",
        "        tiene la mayor activación.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Devolver el vector de derivadas parciales \\partial C_x /\n",
        "        \\partial para las activaciones de salida.\"\"\"\n",
        "        return (output_activations-y)\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    \"\"\"Función sigmoide.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivada de la función sigmoide.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "4dCdj-oQyNux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probando la red neuronal con el conjunto MNIST"
      ],
      "metadata": {
        "id": "sypaZQC7yWnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s7iCMCV8yK_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=mnist.load_data()\n",
        "dat=np.array(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGaC1aGK2I3S",
        "outputId": "ed9671f6-3c96-4c8c-a67d-8b43c9d33bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-84c6aeb0d13b>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  dat=np.array(dataset)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHRIDHC5V7EC",
        "outputId": "74f5aa09-994e-4d82-e6ac-052585804d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZp-vx0_5WCR",
        "outputId": "f920ad61-d0c7-4a27-b207-f824b83bfabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#El primer elemento es de la forma (primer elemento, segundo elemento)\n",
        "#dataset[0]\n"
      ],
      "metadata": {
        "id": "i6ZkvjiU5BxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat.shape #Una matriz con dos filas y dos columnas ¿Pero qué hay en cada celda?,\n",
        "# [[primer elemento,segundo elemento],[tercer elemento,cuarto elemento]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFlPxSUJ2lYI",
        "outputId": "bdf924aa-30a4-455d-c7b1-4c5895ad9f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dat[0,0].shape# primer elemento contiene 60,000 matrices de 28x28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukl39YfVUVf0",
        "outputId": "5e51a486-c0b4-4d58-f2a6-8a654001bb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dat[0,1].shape#segundo elemento contiene un vector de 60,000 elementos\n",
        "#indica númericamente el digito que representa cada matriz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESaeKyUFVVik",
        "outputId": "48b22c37-ad82-4428-f3c8-8238ad87e801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#contiene lo mismo que dat[0,0] y dat[0,1], pero con 10,000 elementos\n",
        "dat[1,0].shape\n",
        "dat[1,1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMZaBJf3Vq1o",
        "outputId": "8971db86-1cb4-476b-c8c0-142c5ce5241d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)#Un tupla con dos elementos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPK9I6c0V2eX",
        "outputId": "aafdaf42-b49d-4cdf-ba3a-b7712dbc7dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#El primer elemento es una tupla que contiene como primer elemento el conjunto de matrices\n",
        "#y como segundo elemento el vector con digitos de los 60,000 elementos\n",
        "#dataset[0]\n",
        "#igual para el segundo elemento de la tupla, pero con 10,000 elementos\n",
        "#dataset[1]\n"
      ],
      "metadata": {
        "id": "T9jmIMbFWQbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=dataset"
      ],
      "metadata": {
        "id": "vrEhaoZ2W1aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorized_result(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ],
      "metadata": {
        "id": "0GUocggDuwab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=[np.reshape(k,(784,1)) for k in x_train]\n",
        "y_train=[vectorized_result(k) for k in y_train]\n",
        "X_train=zip(x_train,y_train)\n",
        "x_test=[np.reshape(k,(784,1)) for k in x_test]\n",
        "X_test=zip(x_test,y_test)"
      ],
      "metadata": {
        "id": "kTZR6AyxXCtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rd1=Network([784,30,10])\n",
        "rd1.SGD(X_train,30,10,0.1,X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "7bnBxXW-XyXS",
        "outputId": "8acb0775-04a2-4e77-abb5-dc5f0a39b9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-67331d1b2f79>:160: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0/(1.0+np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 3282 / 10000\n",
            "Epoch 1 : 4784 / 10000\n",
            "Epoch 2 : 6202 / 10000\n",
            "Epoch 3 : 7270 / 10000\n",
            "Epoch 4 : 7433 / 10000\n",
            "Epoch 5 : 7307 / 10000\n",
            "Epoch 6 : 7752 / 10000\n",
            "Epoch 7 : 7698 / 10000\n",
            "Epoch 8 : 7895 / 10000\n",
            "Epoch 9 : 7793 / 10000\n",
            "Epoch 10 : 7748 / 10000\n",
            "Epoch 11 : 7483 / 10000\n",
            "Epoch 12 : 7761 / 10000\n",
            "Epoch 13 : 8153 / 10000\n",
            "Epoch 14 : 7927 / 10000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-0f365a24f06f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrd1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-67331d1b2f79>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 for k in range(0, n, mini_batch_size)]\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} : {} / {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-67331d1b2f79>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, mini_batch, eta)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-67331d1b2f79>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# list to store all the z vectors, layer by layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mzs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probando red neuronal con nuevo optimizador, función softmax y Cross-Entropy"
      ],
      "metadata": {
        "id": "6K3OTtxBYnt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código de red con optimizador RMs Prop"
      ],
      "metadata": {
        "id": "1VGKcMuXY9dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Libraries\n",
        "# Standard library\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "class Network1(object):\n",
        "\n",
        "    def __init__(self, sizes,beta):\n",
        "        \"\"\"La lista sizes contiene el número de neuronas en las capas\n",
        "        respectivas de la red neuronal. Por ejemplo, si la lista fuera\n",
        "        [2, 3, 1], entonces sería una red de tres capas, con la primera\n",
        "        capa conteniendo 2 neuronas, la segunda capa 3 neuronas y la\n",
        "        tercera capa 1 neurona. Los sesgos (biases) y pesos (weights) de\n",
        "        la red se inicializan de forma aleatoria, utilizando una\n",
        "        distribución Gaussiana con media 0 y varianza 1.\n",
        "        Es importante destacar que se asume que la primera\n",
        "        capa es una capa de entrada y, por convención, no estableceremos\n",
        "        ningún sesgo para esas neuronas, ya que los sesgos solo se\n",
        "        utilizan para calcular las salidas de las capas posteriores.\"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.beta=beta\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Devuelve la salida de la red si a es la entrada.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            test_data=None):\n",
        "        \"\"\"Entrena la red neuronal utilizando el descenso de gradiente\n",
        "        estocástico por lotes pequeños (mini-batch). Los datos de\n",
        "        entrenamiento, representados por la lista de tuplas (x, y),\n",
        "        contienen las entradas de entrenamiento y las salidas deseadas.\n",
        "        Los demás parámetros que no son opcionales son autoexplicativos.\n",
        "        Si se proporciona test_data, entonces la red se evaluará con\n",
        "        respecto a los datos de prueba después de cada época y se\n",
        "        imprimirá el progreso parcial. Esto es útil para hacer un\n",
        "        seguimiento del progreso, pero ralentiza sustancialmente el\n",
        "        proceso. En resumen, esta función entrena la red neuronal\n",
        "        utilizando el método de descenso de gradiente estocástico por\n",
        "        lotes pequeños, ajustando los pesos y sesgos de la red en función\n",
        "        de los datos de entrenamiento proporcionados. Además,\n",
        "        si se proporcionan datos de prueba, se evalúa el rendimiento\n",
        "        de la red en los datos de prueba después de cada época para\n",
        "        realizar un seguimiento de su mejora a lo largo del tiempo.\"\"\"\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Actualiza los pesos y sesgos de la red aplicando el descenso\n",
        "        de gradiente utilizando la retropropagación (backpropagation) a\n",
        "        un solo mini lote. El parámetro mini_batch es una lista de tuplas\n",
        "        (x, y), donde x representa las entradas y y las salidas deseadas\n",
        "        correspondientes a ese mini lote. El parámetro eta representa\n",
        "        la tasa de aprendizaje (learning rate).\n",
        "        \"\"\"\n",
        "        e=10**-9\n",
        "        g_2_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        g_2_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        nabla_b_2=[np.square(k) for k in nabla_b]\n",
        "        nabla_w_2=[np.square(k) for k in nabla_w]\n",
        "        g_2_b=[self.beta*i+(1-self.beta)*j for i,j in zip(g_2_b,nabla_b_2)]\n",
        "        g_2_w=[self.beta*i+(1-self.beta)*j for i,j in zip(g_2_w,nabla_w_2)]\n",
        "        d_b=[np.sqrt(ib+np.full(ib.shape,e)) for ib in g_2_b]\n",
        "        d_w=[np.sqrt(iw+np.full(iw.shape,e)) for iw in g_2_w]\n",
        "\n",
        "        self.weights = [w-eta*(nw/dw)\n",
        "                        for w, dw,nw in zip(self.weights, d_w,nabla_w)]\n",
        "        self.biases = [b-eta*(nb/db)\n",
        "                       for b, db, nb in zip(self.biases,d_b, nabla_b)]\n",
        "\n",
        "\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Devuelve una tupla (nabla_b, nabla_w) que representa el\n",
        "        gradiente para la función de costo C_x. nabla_b y nabla_w son\n",
        "        listas de matrices de numpy, similares a self.biases y self.weights,\n",
        "        pero se refieren al gradiente de la función de costo en relación\n",
        "        con los sesgos y pesos de la red neuronal, respectivamente..\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Note that the variable l in the loop below is used a little\n",
        "        # differently to the notation in Chapter 2 of the book.  Here,\n",
        "        # l = 1 means the last layer of neurons, l = 2 is the\n",
        "        # second-last layer, and so on.  It's a renumbering of the\n",
        "        # scheme in the book, used here to take advantage of the fact\n",
        "        # that Python can use negative indices in lists.\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Devuelve el número de entradas de prueba para las cuales la red\n",
        "        neuronal produce el resultado correcto. Se asume que la salida de\n",
        "        la red neuronal es el índice de la neurona en la capa final que\n",
        "        tiene la mayor activación.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Devolver el vector de derivadas parciales \\partial C_x /\n",
        "        \\partial para las activaciones de salida.\"\"\"\n",
        "        return (output_activations-y)\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    \"\"\"Función sigmoide.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivada de la función sigmoide.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "xs2Vm6asY1Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rd1=Network1([784,30,10],0.1)\n",
        "rd1.SGD(X_train,30,10,0.1,X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pHhqcW2Nc6w",
        "outputId": "80a9c8ef-114e-4390-86c0-1f9ba6994aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-6a40b9ba6867>:159: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0/(1.0+np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 6062 / 10000\n",
            "Epoch 1 : 6920 / 10000\n",
            "Epoch 2 : 7731 / 10000\n",
            "Epoch 3 : 7702 / 10000\n",
            "Epoch 4 : 7682 / 10000\n",
            "Epoch 5 : 7736 / 10000\n",
            "Epoch 6 : 8032 / 10000\n",
            "Epoch 7 : 7937 / 10000\n",
            "Epoch 8 : 7734 / 10000\n",
            "Epoch 9 : 7633 / 10000\n",
            "Epoch 10 : 8113 / 10000\n",
            "Epoch 11 : 8067 / 10000\n",
            "Epoch 12 : 8182 / 10000\n",
            "Epoch 13 : 8273 / 10000\n",
            "Epoch 14 : 8242 / 10000\n",
            "Epoch 15 : 8355 / 10000\n",
            "Epoch 16 : 8415 / 10000\n",
            "Epoch 17 : 8473 / 10000\n",
            "Epoch 18 : 8390 / 10000\n",
            "Epoch 19 : 8456 / 10000\n",
            "Epoch 20 : 8503 / 10000\n",
            "Epoch 21 : 8531 / 10000\n",
            "Epoch 22 : 8475 / 10000\n",
            "Epoch 23 : 8504 / 10000\n",
            "Epoch 24 : 8387 / 10000\n",
            "Epoch 25 : 8577 / 10000\n",
            "Epoch 26 : 8646 / 10000\n",
            "Epoch 27 : 8608 / 10000\n",
            "Epoch 28 : 8565 / 10000\n",
            "Epoch 29 : 8707 / 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función de costo Cross-Entropy y función de activación softmax en última capa, y en las demás una función sigmoide"
      ],
      "metadata": {
        "id": "kcr0Ifi5JuK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network2(object):\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"La lista sizes contiene el número de neuronas en las capas\n",
        "        respectivas de la red neuronal. Por ejemplo, si la lista fuera\n",
        "        [2, 3, 1], entonces sería una red de tres capas, con la primera\n",
        "        capa conteniendo 2 neuronas, la segunda capa 3 neuronas y la\n",
        "        tercera capa 1 neurona. Los sesgos (biases) y pesos (weights) de\n",
        "        la red se inicializan de forma aleatoria, utilizando una\n",
        "        distribución Gaussiana con media 0 y varianza 1.\n",
        "        Es importante destacar que se asume que la primera\n",
        "        capa es una capa de entrada y, por convención, no estableceremos\n",
        "        ningún sesgo para esas neuronas, ya que los sesgos solo se\n",
        "        utilizan para calcular las salidas de las capas posteriores.\"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Devuelve la salida de la red si a es la entrada.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            test_data=None):\n",
        "        \"\"\"Entrena la red neuronal utilizando el descenso de gradiente\n",
        "        estocástico por lotes pequeños (mini-batch). Los datos de\n",
        "        entrenamiento, representados por la lista de tuplas (x, y),\n",
        "        contienen las entradas de entrenamiento y las salidas deseadas.\n",
        "        Los demás parámetros que no son opcionales son autoexplicativos.\n",
        "        Si se proporciona test_data, entonces la red se evaluará con\n",
        "        respecto a los datos de prueba después de cada época y se\n",
        "        imprimirá el progreso parcial. Esto es útil para hacer un\n",
        "        seguimiento del progreso, pero ralentiza sustancialmente el\n",
        "        proceso. En resumen, esta función entrena la red neuronal\n",
        "        utilizando el método de descenso de gradiente estocástico por\n",
        "        lotes pequeños, ajustando los pesos y sesgos de la red en función\n",
        "        de los datos de entrenamiento proporcionados. Además,\n",
        "        si se proporcionan datos de prueba, se evalúa el rendimiento\n",
        "        de la red en los datos de prueba después de cada época para\n",
        "        realizar un seguimiento de su mejora a lo largo del tiempo.\"\"\"\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Actualiza los pesos y sesgos de la red aplicando el descenso\n",
        "        de gradiente utilizando la retropropagación (backpropagation) a\n",
        "        un solo mini lote. El parámetro mini_batch es una lista de tuplas\n",
        "        (x, y), donde x representa las entradas y y las salidas deseadas\n",
        "        correspondientes a ese mini lote. El parámetro eta representa\n",
        "        la tasa de aprendizaje (learning rate).\n",
        "        \"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Devuelve una tupla (nabla_b, nabla_w) que representa el\n",
        "        gradiente para la función de costo C_x. nabla_b y nabla_w son\n",
        "        listas de matrices de numpy, similares a self.biases y self.weights,\n",
        "        pero se refieren al gradiente de la función de costo en relación\n",
        "        con los sesgos y pesos de la red neuronal, respectivamente..\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        n=0\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            n=n+1\n",
        "            if n==self.num_layers-1:\n",
        "              activation=np.exp(z)/(np.sum(np.exp(zs[-1])))# Implementamos la función de activación softmax\n",
        "              #En la última capa\n",
        "              activations.append(activation)\n",
        "            else:\n",
        "              activation = sigmoid(z)\n",
        "              activations.append(activation)\n",
        "\n",
        "\n",
        "\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1],y)#delta de la última capa con función de costo cross-entropy y capa softmax\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Note that the variable l in the loop below is used a little\n",
        "        # differently to the notation in Chapter 2 of the book.  Here,\n",
        "        # l = 1 means the last layer of neurons, l = 2 is the\n",
        "        # second-last layer, and so on.  It's a renumbering of the\n",
        "        # scheme in the book, used here to take advantage of the fact\n",
        "        # that Python can use negative indices in lists.\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Devuelve el número de entradas de prueba para las cuales la red\n",
        "        neuronal produce el resultado correcto. Se asume que la salida de\n",
        "        la red neuronal es el índice de la neurona en la capa final que\n",
        "        tiene la mayor activación.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Devolver el vector de derivadas parciales \\partial C_x /\n",
        "        \\partial para las activaciones de salida.\"\"\"\n",
        "        return (output_activations-y)\n",
        "\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    \"\"\"Función sigmoide.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivada de la función sigmoide.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jdcGBE02J526"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rd1=Network2([784,30,10])\n",
        "rd1.SGD(X_train,30,10,0.1,X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "wd5Gd0aoS2pU",
        "outputId": "c9700a77-3a71-4d11-a979-88c726788174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-8b969fd3d29a>:147: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0/(1.0+np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 4697 / 10000\n",
            "Epoch 1 : 6092 / 10000\n",
            "Epoch 2 : 5097 / 10000\n",
            "Epoch 3 : 4574 / 10000\n",
            "Epoch 4 : 6340 / 10000\n",
            "Epoch 5 : 6079 / 10000\n",
            "Epoch 6 : 6743 / 10000\n",
            "Epoch 7 : 6119 / 10000\n",
            "Epoch 8 : 6067 / 10000\n",
            "Epoch 9 : 6234 / 10000\n",
            "Epoch 10 : 5444 / 10000\n",
            "Epoch 11 : 5815 / 10000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-f9cfaece6d42>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrd1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNetwork2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-8b969fd3d29a>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 for k in range(0, n, mini_batch_size)]\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} : {} / {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-8b969fd3d29a>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, mini_batch, eta)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-8b969fd3d29a>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mnabla_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mnabla_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probando la red neuronal para con otro conjunto de datos"
      ],
      "metadata": {
        "id": "1fFMsG4rGWeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos una gráfica de Epochs vs Loss para cada época"
      ],
      "metadata": {
        "id": "LOxfX_7fyf4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %load network.py\n",
        "\n",
        "\"\"\"\n",
        "network.py\n",
        "~~~~~~~~~~\n",
        "IT WORKS\n",
        "\n",
        "A module to implement the stochastic gradient descent learning\n",
        "algorithm for a feedforward neural network.  Gradients are calculated\n",
        "using backpropagation.  Note that I have focused on making the code\n",
        "simple, easily readable, and easily modifiable.  It is not optimized,\n",
        "and omits many desirable features.\n",
        "\"\"\"\n",
        "\n",
        "#### Libraries\n",
        "# Standard library\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "class Network3(object):\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"La lista sizes contiene el número de neuronas en las capas\n",
        "        respectivas de la red neuronal. Por ejemplo, si la lista fuera\n",
        "        [2, 3, 1], entonces sería una red de tres capas, con la primera\n",
        "        capa conteniendo 2 neuronas, la segunda capa 3 neuronas y la\n",
        "        tercera capa 1 neurona. Los sesgos (biases) y pesos (weights) de\n",
        "        la red se inicializan de forma aleatoria, utilizando una\n",
        "        distribución Gaussiana con media 0 y varianza 1.\n",
        "        Es importante destacar que se asume que la primera\n",
        "        capa es una capa de entrada y, por convención, no estableceremos\n",
        "        ningún sesgo para esas neuronas, ya que los sesgos solo se\n",
        "        utilizan para calcular las salidas de las capas posteriores.\"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Devuelve la salida de la red si a es la entrada.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            test_data=None):\n",
        "        \"\"\"Entrena la red neuronal utilizando el descenso de gradiente\n",
        "        estocástico por lotes pequeños (mini-batch). Los datos de\n",
        "        entrenamiento, representados por la lista de tuplas (x, y),\n",
        "        contienen las entradas de entrenamiento y las salidas deseadas.\n",
        "        Los demás parámetros que no son opcionales son autoexplicativos.\n",
        "        Si se proporciona test_data, entonces la red se evaluará con\n",
        "        respecto a los datos de prueba después de cada época y se\n",
        "        imprimirá el progreso parcial. Esto es útil para hacer un\n",
        "        seguimiento del progreso, pero ralentiza sustancialmente el\n",
        "        proceso. En resumen, esta función entrena la red neuronal\n",
        "        utilizando el método de descenso de gradiente estocástico por\n",
        "        lotes pequeños, ajustando los pesos y sesgos de la red en función\n",
        "        de los datos de entrenamiento proporcionados. Además,\n",
        "        si se proporcionan datos de prueba, se evalúa el rendimiento\n",
        "        de la red en los datos de prueba después de cada época para\n",
        "        realizar un seguimiento de su mejora a lo largo del tiempo.\"\"\"\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        cost_history=[]\n",
        "\n",
        "\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "            # Calcula y almacena la función de costo en esta época\n",
        "\n",
        "            Cx=[]\n",
        "            for x in training_data:\n",
        "\n",
        "              aL = self.feedforward(x[0])\n",
        "              Cost=(aL-x[1])*(aL-x[1])\n",
        "              Cx.append(Cost.mean())\n",
        "            cost_history.append(sum(Cx)/len(Cx))\n",
        "            print('Loss',cost_history[-1])\n",
        "\n",
        "            plt.plot(range(j+1),cost_history)\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Actualiza los pesos y sesgos de la red aplicando el descenso\n",
        "        de gradiente utilizando la retropropagación (backpropagation) a\n",
        "        un solo mini lote. El parámetro mini_batch es una lista de tuplas\n",
        "        (x, y), donde x representa las entradas y y las salidas deseadas\n",
        "        correspondientes a ese mini lote. El parámetro eta representa\n",
        "        la tasa de aprendizaje (learning rate).\n",
        "        \"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Devuelve una tupla (nabla_b, nabla_w) que representa el\n",
        "        gradiente para la función de costo C_x. nabla_b y nabla_w son\n",
        "        listas de matrices de numpy, similares a self.biases y self.weights,\n",
        "        pero se refieren al gradiente de la función de costo en relación\n",
        "        con los sesgos y pesos de la red neuronal, respectivamente..\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Note that the variable l in the loop below is used a little\n",
        "        # differently to the notation in Chapter 2 of the book.  Here,\n",
        "        # l = 1 means the last layer of neurons, l = 2 is the\n",
        "        # second-last layer, and so on.  It's a renumbering of the\n",
        "        # scheme in the book, used here to take advantage of the fact\n",
        "        # that Python can use negative indices in lists.\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Devuelve el número de entradas de prueba para las cuales la red\n",
        "        neuronal produce el resultado correcto. Se asume que la salida de\n",
        "        la red neuronal es el índice de la neurona en la capa final que\n",
        "        tiene la mayor activación.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Devolver el vector de derivadas parciales \\partial C_x /\n",
        "        \\partial para las activaciones de salida.\"\"\"\n",
        "        return (output_activations-y)\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    \"\"\"Función sigmoide.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivada de la función sigmoide.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "Q0TSN5o9yd8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_circles\n",
        "n=600\n",
        "p=2\n",
        "X,Y=make_circles(n_samples=n,factor=0.5,noise=0.08)\n",
        "plt.scatter(X[Y==0,0],X[Y==0,1], c='skyblue')\n",
        "plt.scatter(X[Y==1,0],X[Y==1,1], c='salmon')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "TBhzs-Jzbv7z",
        "outputId": "512e2aea-df04-4cfc-eb93-f6fa11204e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Separamos los datos de entranmiento con los datos de prueba\n",
        "r=X\n",
        "t=Y\n",
        "n=60\n",
        "y_test=t.copy()\n",
        "y_train=t.copy()\n",
        "i=j=k=0\n",
        "for k in range(t.size):\n",
        "  v=t[k]\n",
        "  if (v==0) & (i<n):\n",
        "    t[k]=3\n",
        "    i=i+1\n",
        "  if (v==1)& (j<n):\n",
        "    t[k]=3\n",
        "    j=j+1\n",
        "  if j+i==2*n:\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "x_test=r[t==3]\n",
        "y_test=y_test[t==3]\n",
        "\n",
        "x_train=r[t!=3]\n",
        "y_train=y_train[t!=3]"
      ],
      "metadata": {
        "id": "0lnALu4vfs7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OneHotEncoding(x):\n",
        "  v=np.zeros((2,1))\n",
        "  v[x]=1\n",
        "  return v"
      ],
      "metadata": {
        "id": "Jebw8SgfhNMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_trainv=[np.reshape(k,(2,1)) for k in x_train]\n",
        "y_trainv=[OneHotEncoding(k) for k in y_train]\n",
        "train=zip(x_trainv,y_trainv)\n",
        "x_testv=[np.reshape(k,(2,1)) for k in x_test]\n",
        "test=zip(x_testv,y_test)"
      ],
      "metadata": {
        "id": "wpFzc88gcwpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2cxEaZStOHF",
        "outputId": "bb8faa7a-6cb7-4bb1-b5c0-95a81a40ea9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX5DMysQtbLI",
        "outputId": "856cd38e-6cb2-4015-e4b8-87e5d27379c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l=Network1([2,15,2])\n",
        "l.SGD(train, 10, 5, 1.0, test_data=test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "KHKjt4jrjUO9",
        "outputId": "be2d9a0e-69ed-495f-f131-609df0cd2451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 complete\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-629062314663>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNetwork1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-94-5820a814229f>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m     98\u001b[0m               \u001b[0mCost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m               \u001b[0mCx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mcost_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rn1=Network([784,30,10])\n",
        "rn1.SGD(X_train,30,10,0.1,X_test)"
      ],
      "metadata": {
        "id": "1NtOSVLZ-Iua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=dat[0,0].shape #Tiene 60,0000 elementos, estos elementos son matrices"
      ],
      "metadata": {
        "id": "tuW54IUj2w5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=dat[0,1].shape #Tiene 60,000 elementos, estos son los elementos numéricos que indica que digito ilustra cada imagen"
      ],
      "metadata": {
        "id": "miZS0cLq3gs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lo mismo que antes pero con 10,000 elementos(Estos los vamos a utilizar como elementos de prueba)\n",
        "x=dat[1,0].shape\n",
        "y=dat[1,1]"
      ],
      "metadata": {
        "id": "HyD5sLe826V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=dataset"
      ],
      "metadata": {
        "id": "Wr6Da8m14Eha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}