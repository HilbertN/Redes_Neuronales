{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HilbertN/Redes_Neuronales/blob/main/RN10_Regresion_GradientTape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('default')"
      ],
      "metadata": {
        "id": "coh2rbJ2kA_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blood Pressure data\n",
        "x = [22, 41, 52, 23, 41, 54, 24, 46, 56, 27, 47, 57, 28, 48, 58,  9,\n",
        "     49, 59, 30, 49, 63, 32, 50, 67, 33, 51, 71, 35, 51, 77, 40, 51, 81]\n",
        "y = [131, 139, 128, 128, 171, 105, 116, 137, 145, 106, 111, 141, 114,\n",
        "     115, 153, 123, 133, 157, 117, 128, 155, 122, 183,\n",
        "     176,  99, 130, 172, 121, 133, 178, 147, 144, 217]\n",
        "x = np.asarray(x, np.float32)\n",
        "y = np.asarray(y, np.float32)"
      ],
      "metadata": {
        "id": "Izg3SdtSkS0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(a, b):\n",
        "  y_hat = a*x + b\n",
        "  return tf.reduce_mean((y_hat - y)**2)"
      ],
      "metadata": {
        "id": "ZWBcfdwGkEiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se muestra como tensorflow V2 usa \"eager execution\". Es decir, asigna valores a los nodos del la Grafica que representa la funcion tan pronto como puede."
      ],
      "metadata": {
        "id": "2VokOjXDRr9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.Variable(0.0)\n",
        "b = tf.Variable(139.0)\n",
        "loss(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nylA9pxk7Dj",
        "outputId": "b30febb1-b7dd-4fdc-e608-076691af4eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=673.4545>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.Variable(0.0)\n",
        "b = tf.Variable(139.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  loss_val = loss(a,b)\n",
        "  print(\"Loss at \", loss_val)\n",
        "grad_a, grad_b = tape.gradient(loss_val, [a,b])\n",
        "print(grad_a, grad_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8Ml8jnJlByq",
        "outputId": "bfdf6013-0ca4-491a-bf53-bf78c6f8b16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at  tf.Tensor(673.4545, shape=(), dtype=float32)\n",
            "tf.Tensor(-553.09094, shape=(), dtype=float32) tf.Tensor(0.727273, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(139.0)\n",
        "eta = 0.0004\n",
        "start= time()\n",
        "for i in range(8000):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_hat = a*x + b\n",
        "    loss = tf.reduce_mean((y_hat - y)**2)\n",
        "  grad_a, grad_b  = tape.gradient(loss, [a,b])\n",
        "  a = tf.Variable(a - eta * grad_a)\n",
        "  b = tf.Variable(b - eta * grad_b)\n",
        "  if (i % 5000 == 0):\n",
        "      t = time() - start\n",
        "      print(\"Epoch:\",i, \"slope=\",a.numpy(),\"intercept=\",b.numpy(),\"gradient_a\", grad_a.numpy(), \"gradient_b\",grad_b.numpy(), \"mse=\", loss.numpy(), \"time for 1000 epochs \", t/5.)\n",
        "      start = time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMCHiVvflY-P",
        "outputId": "1ec0e2ec-df73-4b6e-9d78-627df0ebe6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 slope= 0.22123638 intercept= 138.99971 gradient_a -553.09094 gradient_b 0.727273 mse= 673.4545 time for 1000 epochs  0.0016683101654052734\n",
            "Epoch: 5000 slope= 0.47009143 intercept= 120.60784 gradient_a -0.14053345 gradient_b 7.3059855 mse= 469.57272 time for 1000 epochs  4.260841751098633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para mejorar el tiempo de cómputo, se puede usar \"Lazy Evaluation\" que es lo contrario de \"Eager Evaluation\". Es decir, no asigna valores en la Grafica que representa la función y por lo tanto la \"misma gráfica\" se puede usar en cada evaluación particular. Esto, elimina el costo computacional de crear una nueva gráfica cada vez que se evalúa la funcion.\n",
        "\n",
        "Para esta \"configuración\" simplemente es neceario usar el decorador @tf.function."
      ],
      "metadata": {
        "id": "iHWSExuWWp4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start= time()\n",
        "a  = tf.Variable(0.0)\n",
        "b = tf.Variable(139.0)\n",
        "eta = 0.0004\n",
        "\n",
        "@tf.function #Will tell tf to build a graph from this code\n",
        "def train_step():\n",
        "    y_hat = a*x + b\n",
        "    loss = tf.reduce_mean((y_hat - y)**2)\n",
        "    grad_a, grad_b  = tape.gradient(loss, [a,b])\n",
        "    a.assign(a - eta * grad_a)\n",
        "    b.assign(b - eta * grad_b)\n",
        "\n",
        "for i in range(8000):\n",
        "  with tf.GradientTape() as tape: #Record the gradients from now on\n",
        "    train_step()\n",
        "    if (i % 5000 == 0):\n",
        "        t = time() - start\n",
        "        print(\"Epoch:\",i, \"slope=\",a.numpy(),\"intercept=\",b.numpy(),\"gradient_a\", \"mse=\", loss.numpy(), \"time for 1000 epochs \", t/5.)\n",
        "        start = time()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1GU_1rGm4VS",
        "outputId": "db59577f-8f5d-4220-abd2-3857445b2b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 slope= 0.22123638 intercept= 138.99971 gradient_a mse= 673.4545 time for 1000 epochs  0.146035099029541\n",
            "Epoch: 5000 slope= 0.47009143 intercept= 120.60784 gradient_a mse= 673.4545 time for 1000 epochs  0.5113940238952637\n"
          ]
        }
      ]
    }
  ]
}